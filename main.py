# main.py
# Entry point for the translation pipeline
# Architecture: Chapter-atomic, deterministic, debug-first
# BRD v1.5 compliant (Translator + State + Editor)

from engine.chapter_classifier import ChapterType
from engine.translation_engine import TranslationEngine
from engine.glossary_engine import GlossaryEngine
from engine.summary_engine import SummaryEngine
from engine.character_engine import CharacterEngine
from engine.in_chapter_state import InChapterState
from engine.html_block_extractor import extract_html_blocks
from engine.html_rebuilder import rebuild_html_blocks
from engine import state_manager
from engine.checkpoint_manager import load_checkpoint, mark_done
from engine.editor_engine import EditorEngine

from epub.epub_loader import load_epub
from epub.epub_writer import write_epub

from utils.logger import log
from utils.inspect import print_chapter_list

import uuid
import ebooklib
from bs4 import BeautifulSoup
from typing import List
import json
import asyncio  # Import asyncio r√µ r√†ng

INPUT_EPUB = "input.epub"
OUTPUT_EPUB = "output_bilingual.epub"

# L∆∞u √Ω: MAX_BLOCKS_PER_CHUNK c≈© kh√¥ng c√≤n d√πng ƒë·ªÉ chia chunk,
# nh∆∞ng gi·ªØ l·∫°i h·∫±ng s·ªë INTRA_CONTEXT_BLOCKS ƒë·ªÉ l·∫•y ng·ªØ c·∫£nh.
INTRA_CONTEXT_BLOCKS = 200

def sanitize_book_structure(book):
    """
    H√†m n√†y r√† so√°t to√†n b·ªô s√°ch v√† g√°n ID gi·∫£ cho b·∫•t k·ª≥ th√†nh ph·∫ßn n√†o b·ªã thi·∫øu ID.
    Gi√∫p tr√°nh l·ªói 'NoneType' khi ghi file.
    """
    log("üßπ B·∫ÆT ƒê·∫¶U R√Ä SO√ÅT C·∫§U TR√öC S√ÅCH (SANITIZING)...")
    count_fixed = 0

    # 1. R√† so√°t danh s√°ch file (Items)
    for item in book.get_items():
        if not item.id:  # N·∫øu ID b·ªã None ho·∫∑c r·ªóng
            new_id = f"fixed_item_{uuid.uuid4().hex[:8]}"
            item.set_id(new_id)
            log(f"   üîß ƒê√£ s·ª≠a Item thi·∫øu ID: {item.file_name} -> {new_id}")
            count_fixed += 1

    # 2. R√† so√°t M·ª•c l·ª•c (TOC) - ƒê·ªá quy
    def fix_toc_node(node):
        nonlocal count_fixed
        # N·∫øu node l√† m·ªôt list/tuple (Section con), duy·ªát ƒë·ªá quy
        if isinstance(node, (list, tuple)):
            for child in node:
                fix_toc_node(child)
        # N·∫øu node l√† m·ªôt Link object (th∆∞·ªùng g·∫∑p trong ebooklib)
        elif hasattr(node, 'uid'):
            if not node.uid:
                new_uid = f"fixed_toc_{uuid.uuid4().hex[:8]}"
                node.uid = new_uid
                title = getattr(node, 'title', 'No Title')
                log(f"   üîß ƒê√£ s·ª≠a TOC Node thi·∫øu UID: '{title}' -> {new_uid}")
                count_fixed += 1

    fix_toc_node(book.toc)

    if count_fixed > 0:
        log(f"‚úÖ ƒê√É S·ª¨A XONG {count_fixed} L·ªñI C·∫§U TR√öC.")
    else:
        log("‚úÖ C·∫§U TR√öC S√ÅCH ·ªîN ƒê·ªäNH. KH√îNG C√ì L·ªñI ID.")

def build_glossary_rules(*, base_glossary: dict, delta_terms: list) -> str:
    entries = []
    for e in base_glossary.get("entries", []):
        entries.append(f'- "{e["source"]}" ‚Üí "{e["target"]}"')
    for e in delta_terms:
        entries.append(f'- "{e["source"]}" ‚Üí "{e["target"]}"')

    if not entries:
        return ""

    return (
            "GLOSSARY RULES (HARD CONSTRAINT):\n"
            "- Every source term MUST be translated EXACTLY as specified.\n"
            "- Do NOT paraphrase or localize glossary terms.\n\n"
            "Glossary:\n" + "\n".join(entries)
    )


async def run():
    log("START TRANSLATION PIPELINE")

    book = load_epub(INPUT_EPUB)

    total_chapters = print_chapter_list(book)
    first_narrative_index = int(input("üëâ Enter FIRST_NARRATIVE_INDEX: "))
    last_chapter_index = int(input("üëâ Enter LAST_CHAPTER_INDEX: "))

    engine = TranslationEngine()
    editor_engine = EditorEngine()
    glossary_engine = GlossaryEngine()
    summary_engine = SummaryEngine(engine.client)
    character_engine = CharacterEngine(engine.client)

    glossary = state_manager.load_glossary()
    summary = state_manager.load_summary()
    characters = state_manager.load_characters()

    done_chapters = load_checkpoint()

    for idx, item in enumerate(book.get_items_of_type(ebooklib.ITEM_DOCUMENT)):

        if idx > last_chapter_index:
            break
        if idx in done_chapters:
            # S·ª¨A T·∫†I ƒê√ÇY: Thay v√¨ ch·ªâ continue, h√£y n·∫°p l·∫°i b·∫£n d·ªãch c≈©
            log(f"CHECKPOINT: skip translation for chapter {idx}, loading previous version")

            # Gi·∫£ s·ª≠ state_manager c·ªßa b·∫°n l∆∞u file HTML t·∫°i 'state/chapters/chapter_{idx}.html'
            # (B·∫°n c·∫ßn ki·ªÉm tra ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c trong state_manager.py c·ªßa b·∫°n)
            cached_html = state_manager.get_chapter_html(idx)

            if cached_html:
                item.set_content(cached_html.encode("utf-8"))
            else:
                log(f"‚ö†Ô∏è Warning: Chapter {idx} marked done but no cache found. Keeping original.")
            continue

        soup = BeautifulSoup(
            item.get_content().decode("utf-8", errors="ignore"),
            "html.parser"
        )

        if idx < first_narrative_index:
            chapter_type = ChapterType.NON_NARRATIVE
        elif idx == first_narrative_index:
            chapter_type = ChapterType.FIRST_NARRATIVE
        else:
            chapter_type = ChapterType.NARRATIVE

        is_narrative = chapter_type != ChapterType.NON_NARRATIVE
        log(f"CHAPTER {idx} TYPE = {chapter_type}")

        en_blocks, html_nodes = extract_html_blocks(soup)
        if not en_blocks:
            mark_done(idx)
            continue

        chapter_text = "\n".join(en_blocks)
        in_state = InChapterState()

        # ---------- GLOSSARY DELTA ----------
        if is_narrative:
            # --- PH·∫¶N S·ª¨A ƒê·ªîI: RETRY & FALLBACK CHO GLOSSARY ---
            ai_text = ""
            glossary_max_retries = 3
            last_glossary_error = None

            for g_attempt in range(1, glossary_max_retries + 1):
                # L·∫ßn cu·ªëi c√πng (l·∫ßn 3) s·∫Ω d√πng fallback model
                current_glossary_model = engine.model_glossary
                if g_attempt == glossary_max_retries:
                    current_glossary_model = "gemini-3-flash-preview"
                    log(f"‚ö†Ô∏è GLOSSARY: Switching to FALLBACK MODEL: {current_glossary_model}")

                try:
                    log(f"GLOSSARY: API call attempt {g_attempt} using [{current_glossary_model}]")
                    ai_text = engine._call_openai(
                        prompt=glossary_engine.build_delta_prompt(
                            current_glossary=glossary,
                            chapter_text=chapter_text,
                        ),
                        model=current_glossary_model,  # S·ª≠ d·ª•ng model ƒë√£ ch·ªçn
                    )
                    if ai_text: break  # Th√†nh c√¥ng th√¨ tho√°t v√≤ng l·∫∑p
                except Exception as e:
                    last_glossary_error = e
                    log(f"‚ö†Ô∏è GLOSSARY API ERROR (Attempt {g_attempt}): {e}")
                    if g_attempt < glossary_max_retries:
                        await asyncio.sleep(2 * g_attempt)

            if not ai_text:
                log(f"‚ùå GLOSSARY FAILED sau {glossary_max_retries} l·∫ßn th·ª≠. B·ªè qua delta ch∆∞∆°ng n√†y.")
                # N·∫øu glossary l·ªói ho√†n to√†n, ta ƒë·ªÉ ai_text l√† m·∫£ng r·ªóng ƒë·ªÉ kh√¥ng l√†m s·∫≠p pipeline
                ai_text = "[]"

            # 2. Parse k·∫øt qu·∫£
            raw_terms = glossary_engine.parse_delta(ai_text)

            # 3. KH·ª¨ TR√ôNG H·ªÜ TH·ªêNG
            existing_sources = {e["source"].lower() for e in glossary.get("entries", [])}
            unique_terms = []
            for term in raw_terms:
                if term["source"].lower() not in existing_sources:
                    unique_terms.append(term)

            if len(raw_terms) > len(unique_terms):
                log(f"‚ö†Ô∏è GLOSSARY: ƒê√£ l·ªçc b·ªè {len(raw_terms) - len(unique_terms)} t·ª´ tr√πng l·∫∑p.")

            # 4. N·∫°p v√†o state
            in_state.add_glossary_terms(unique_terms)

        glossary_rules = (
            build_glossary_rules(
                base_glossary=glossary,
                delta_terms=in_state.glossary_delta,
            ) if is_narrative else ""
        )

        # ---------- TRANSLATION (DYNAMIC CHUNKING UPDATE) ----------
        vi_blocks: List[str] = []

        # 1. Chu·∫©n b·ªã Context String ƒë·ªÉ t√≠nh token n·ªÅn (Static Context)
        # T·∫°o chu·ªói rules ƒë·∫°i t·ª´
        char_rules_str = "\n".join([f"- {c['name']}: {c['vi_pronoun']['default']}" for c in characters])
        summary_json_str = json.dumps(summary, ensure_ascii=False)
        chars_json_str = json.dumps(characters, ensure_ascii=False) if characters else ""

        # ∆Ø·ªõc l∆∞·ª£ng t·ªïng ƒë·ªô d√†i c·ªßa ph·∫ßn Prompt c·ªë ƒë·ªãnh (System prompt + Glossary + Summary...)
        # Engine s·∫Ω d√πng con s·ªë n√†y ƒë·ªÉ bi·∫øt "c√≤n bao nhi√™u ch·ªó tr·ªëng" cho text c·∫ßn d·ªãch.
        static_context_str = (
                glossary_rules +
                f"\n{char_rules_str}\n" +
                summary_json_str +
                chars_json_str +
                "You are a professional literary translator..."  # System Prompt Buffer
        )
        static_len = len(static_context_str)

        # 2. V√≤ng l·∫∑p c·∫Øt chunk ƒë·ªông (Dynamic Loop)
        current_idx = 0
        chunk_counter = 1
        total_blocks_count = len(en_blocks)

        while current_idx < total_blocks_count:
            remaining_blocks = en_blocks[current_idx:]

            # -> G·ªåI ENGINE: T√≠nh to√°n xem n√™n l·∫•y bao nhi√™u block d·ª±a tr√™n token limit
            num_blocks_to_take = engine.calculate_optimal_chunk_size(
                remaining_blocks=remaining_blocks,
                static_context_len=static_len
            )

            current_chunk = remaining_blocks[:num_blocks_to_take]

            # G·ªåI ENGINE: D·ªãch chunk
            vi_chunk = engine.translate_chunk(
                en_blocks=current_chunk,
                glossary_rules=glossary_rules,
                summary=f"CHARACTER PRONOUNS:\n{char_rules_str}\n\nSTORY SUMMARY:\n{summary_json_str}",
                characters=chars_json_str,
                intra_chapter_context=in_state.get_last_chunks(INTRA_CONTEXT_BLOCKS),
                is_narrative=is_narrative,
                chunk_index=chunk_counter,
                total_chunks=0,  # <--- S·ª≠a th√†nh 0 ƒë·ªÉ hi·ªÉn th·ªã log l√† '1/?', '2/?'...
                total_chapter_blocks=total_blocks_count,
            )

            vi_blocks.extend(vi_chunk)
            in_state.add_translated_chunk(vi_chunk)

            # C·∫≠p nh·∫≠t index
            current_idx += num_blocks_to_take
            chunk_counter += 1

        if len(vi_blocks) != len(en_blocks):
            raise RuntimeError("BLOCK COUNT MISMATCH")

        # ---------- SUMMARY + CHARACTER UPDATE ----------
        if is_narrative:
            if chapter_type == ChapterType.FIRST_NARRATIVE:
                in_state.summary_snapshot = summary_engine.init_summary(chapter_text)
                in_state.character_snapshot = character_engine.init_characters(chapter_text)
            else:
                in_state.summary_snapshot = summary_engine.update_summary(summary, chapter_text)
                in_state.character_snapshot = character_engine.update_characters(characters, chapter_text)

        # ---------- EDITOR (CHAPTER LEVEL) ----------
        if is_narrative:
            log(f"EDITOR START chapter {idx}")

            # Flat dict source -> target only (Editor expects this format)
            full_glossary_for_editor = {}
            for e in glossary.get("entries", []):
                full_glossary_for_editor[e["source"]] = e["target"]
            for term in in_state.glossary_delta:
                src = term.get("source")
                tgt = term.get("target")
                if src and tgt:
                    full_glossary_for_editor[src] = tgt

            vi_blocks = await editor_engine.edit_chapter(
                original_blocks=en_blocks,
                draft_vi_blocks=vi_blocks,
                glossary=full_glossary_for_editor
            )
            log(f"EDITOR DONE chapter {idx}")

        # ---------- HTML REBUILD ----------
        rebuild_html_blocks(html_nodes, vi_blocks)
        item.set_content(str(soup).encode("utf-8"))

        # ---------- COMMIT ----------
        state_manager.commit_chapter(idx, in_state, str(soup))
        if is_narrative:

            glossary = state_manager.load_glossary()
            summary = state_manager.load_summary()
            characters = state_manager.load_characters()

        mark_done(idx)

    # === DEBUG: Ki·ªÉm tra book & TOC m·ªôt l·∫ßn sau khi x·ª≠ l√Ω xong t·∫•t c·∫£ ch∆∞∆°ng ===
    print("\n[DEBUG] --- KI·ªÇM TRA BOOK & TOC TR∆Ø·ªöC KHI GHI ---")
    for item in book.get_items():
        if item.get_id() is None:
            print(f"‚ùå ITEM L·ªñI (No ID): Type={type(item)} Name={item.get_name()}")
            new_id = f"fixed_{uuid.uuid4().hex[:8]}"
            item.set_id(new_id)
            print(f"   -> ƒê√£ auto-fix g√°n ID m·ªõi: {new_id}")

    def check_toc(toc_list):
        for node in toc_list:
            if isinstance(node, (list, tuple)):
                check_toc(node)
            elif hasattr(node, 'uid'):
                if node.uid is None:
                    print(f"‚ùå TOC NODE L·ªñI (No UID): Title={getattr(node, 'title', 'N/A')}")
                    if hasattr(node, 'set_id'):
                        node.set_id(f"toc_fixed_{uuid.uuid4().hex[:8]}")
                        print(f"   -> ƒê√£ auto-fix g√°n UID cho TOC node.")
            else:
                print(f"‚ö†Ô∏è C·∫£nh b√°o: Node trong TOC kh√¥ng ph·∫£i Link/Item chu·∫©n: {type(node)}")

    check_toc(book.toc)
    print("[DEBUG] --- K·∫æT TH√öC KI·ªÇM TRA ---\n")

    sanitize_book_structure(book)
    log(f"WRITING TO: {OUTPUT_EPUB}")
    write_epub(OUTPUT_EPUB, book)
    log("PIPELINE DONE")


if __name__ == "__main__":
    asyncio.run(run())