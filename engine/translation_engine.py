# engine/translation_engine.py
# Responsibility:
# - Translate text blocks into Vietnamese
# - NO state mutation
# - Model selection explicit
# - Support rolling intra-chapter context (read-only)
# - Fail-loud, debug-first
# Python 3.9 compatible

import os
import time
import json
import re
import random
from typing import Optional, Dict, List
from utils.logger import log

# === THAY ƒê·ªîI: D√πng th∆∞ vi·ªán Google GenAI g·ªëc ƒë·ªÉ ch·ªânh Safety Settings ===
from google import genai
from google.genai import types
from google.genai import errors

# =========================================================
# CONSTANTS FOR DYNAMIC CHUNKING
# =========================================================
EXPANSION_RATIO = 1.8  # Ti·∫øng Vi·ªát d√†i h∆°n ti·∫øng Anh ~1.8 l·∫ßn
SAFETY_BUFFER = 0.9  # Ch·ªâ d√πng 90% dung l∆∞·ª£ng Output cho ph√©p
HARD_LIMIT_BLOCKS = 999  # Kh√¥ng bao gi·ªù g·ª≠i qu√° 40 ƒëo·∫°n/l·∫ßn
CHARS_PER_TOKEN = 3.5  # ∆Ø·ªõc l∆∞·ª£ng b·∫£o th·ªß (trung b√¨nh l√† 4)


# =========================================================
# PRONOUN RULE BUILDER (HARD CONSTRAINT)
# =========================================================
def build_pronoun_rules(characters: list) -> str:
    """
    Build HARD CONSTRAINT rules for Vietnamese 3rd-person pronouns.
    characters: list of character objects with vi_pronoun
    """
    lines = []

    for c in characters:
        name = c.get("name")
        vi = c.get("vi_pronoun", {}).get("default")

        if name and vi:
            lines.append(f'- "{name}" MUST be referred to as "{vi}"')

    if not lines:
        return ""

    return (
            "CHARACTER PRONOUN RULES (ABSOLUTE):\n"
            "- When translating English third-person references "
            "(he / him / his), you MUST use the Vietnamese pronoun specified below.\n"
            "- You MUST NOT vary pronouns for style.\n"
            "- You MUST NOT replace pronouns with character names or descriptions.\n"
            "- You MUST NOT avoid pronouns by repeating names.\n"
            "- Any pronoun violation INVALIDATES the output.\n\n"
            "Pronoun mapping:\n"
            + "\n".join(lines)
            + "\n"
    )


# =========================================================
# ENGINE
# =========================================================
class TranslationEngine:
    def __init__(self):
        """
        Kh·ªüi t·∫°o Engine d√πng Google GenAI SDK (Official)
        L√Ω do: ƒê·ªÉ t·∫Øt b·ªô l·ªçc n·ªôi dung (BLOCK_NONE) tr√°nh l·ªói PROHIBITED_CONTENT
        """
        # 1. C·∫•u h√¨nh Model (Logic Fallback)
        self.model_primary = "gemini-2.5-flash-lite"
        self.model_fallback = "gemini-3-flash-preview"
        self.model_glossary = "gemini-2.5-flash-lite"

        self.max_retries = 5
        self.timeout_sec = 120

        # 2. L·∫•y API Key
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("‚ùå L·ªñI: Thi·∫øu d√≤ng GOOGLE_API_KEY=... trong file .env")

        # 3. T·∫°o Client Google (Native)
        self.client = genai.Client(api_key=api_key)

        # 4. Cache limit cho t·ª´ng model (Tr√°nh g·ªçi API get_model li√™n t·ª•c)
        self._limit_cache = {}

    # =========================================================
    # NEW: DYNAMIC CHUNKING & TOKEN LOGIC
    # =========================================================
    def _get_model_limit(self, model_name: str) -> int:
        """L·∫•y Output Token Limit c·ªßa model (Lazy load + Cache)"""
        if model_name in self._limit_cache:
            return self._limit_cache[model_name]

        try:
            # SDK m·ªõi: client.models.get(model='models/...')
            full_name = model_name if "models/" in model_name else f"models/{model_name}"
            model_info = self.client.models.get(model=full_name)

            # L·∫•y output limit
            limit = model_info.output_token_limit
            self._limit_cache[model_name] = limit
            log(f"‚ÑπÔ∏è Model Config [{model_name}]: Output Limit = {limit}")
            return limit
        except Exception as e:
            log(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c limit model {model_name}: {e}. D√πng default 8192.")
            # Fallback an to√†n n·∫øu API l·ªói
            default_limit = 8192
            self._limit_cache[model_name] = default_limit
            return default_limit

    def calculate_optimal_chunk_size(
            self,
            remaining_blocks: list[str],
            static_context_len: int
    ) -> int:
        """
        T√≠nh s·ªë block t·ªëi ƒëa g·ª≠i ƒëi ƒë∆∞·ª£c.
        Logic: ∆Ø·ªõc l∆∞·ª£ng c·ª•c b·ªô -> N·∫øu > 80% ng∆∞·ª°ng -> G·ªçi API ƒë·∫øm th·∫≠t.
        """
        # 1. L·∫•y limit c·ªßa model PRIMARY
        raw_limit = self._get_model_limit(self.model_primary)
        safe_limit = int(raw_limit * SAFETY_BUFFER)

        # Input n·ªÅn (Prompt h·ªá th·ªëng + Summary...)
        base_input_est = int(static_context_len / CHARS_PER_TOKEN)

        current_est_tokens = 0
        blocks_to_take = 0
        accumulated_text = ""

        for block in remaining_blocks:
            # A. T√≠nh nh·∫©m (Local Estimate)
            block_len = len(block)
            block_est = int(block_len / CHARS_PER_TOKEN)

            # D·ª± ph√≥ng Output Token = (Input ƒë√£ c√≥ + Block m·ªõi) * H·ªá s·ªë n·ªü
            projected_output = (current_est_tokens + block_est) * EXPANSION_RATIO

            # B. Checkpoint: N·∫øu ∆∞·ªõc l∆∞·ª£ng v∆∞·ª£t qu√° 80% gi·ªõi h·∫°n -> Check k·ªπ b·∫±ng API
            if projected_output > (safe_limit * 0.8):
                try:
                    # G·ªçi API count_tokens (Ch√≠nh x√°c tuy·ªát ƒë·ªëi)
                    test_content = accumulated_text + "\n" + block

                    # SDK m·ªõi: client.models.count_tokens
                    resp = self.client.models.count_tokens(
                        model=self.model_primary,
                        contents=test_content
                    )
                    real_input = resp.total_tokens

                    # T√≠nh output d·ª± ki·∫øn d·ª±a tr√™n s·ªë th·∫≠t
                    real_projected_output = real_input * EXPANSION_RATIO

                    if real_projected_output > safe_limit:
                        log(f"üõë CUT CHUNK (API check): Output {real_projected_output:.0f} > {safe_limit}")
                        break
                except Exception as e:
                    # N·∫øu API l·ªói, tin v√†o ∆∞·ªõc l∆∞·ª£ng v√† d·ª´ng cho an to√†n
                    log(f"‚ö†Ô∏è Count tokens error: {e}")
                    if projected_output > safe_limit:
                        break

            # C. Hard Limit (S·ªë block t·ªëi ƒëa ƒë·ªÉ AI kh√¥ng b·ªã lo·∫°n)
            if blocks_to_take >= HARD_LIMIT_BLOCKS:
                break

            # D. Ch·∫•p nh·∫≠n block
            current_est_tokens += block_est
            accumulated_text += "\n" + block
            blocks_to_take += 1

        return max(1, blocks_to_take)

    # =========================================================
    # LOW-LEVEL API CALL (REPLACED OPENAI WITH GEMINI NATIVE)
    # =========================================================
    def _call_gemini_native(self, *, prompt: str, model: str) -> str:
        """
        G·ªçi tr·ª±c ti·∫øp Google Gemini v·ªõi c·∫•u h√¨nh t·∫Øt to√†n b·ªô Safety Filter.
        T√≠ch h·ª£p logic x·ª≠ l√Ω l·ªói 429 (Quota) v·ªõi Exponential Backoff.
        """
        # Debug ƒë·ªÉ b·∫°n t·ª± ki·ªÉm tra
        print(f"[DEBUG] Model ID g·ª≠i ƒëi: {model}")

        # --- C·∫§U H√åNH QUAN TR·ªåNG: T·∫ÆT B·ªò L·ªåC ---
        safety_settings = [
            types.SafetySetting(
                category="HARM_CATEGORY_HARASSMENT",
                threshold="BLOCK_NONE",
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_HATE_SPEECH",
                threshold="BLOCK_NONE",
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
                threshold="BLOCK_NONE",
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_DANGEROUS_CONTENT",
                threshold="BLOCK_NONE",
            ),
        ]

        # C·∫•u h√¨nh sinh vƒÉn b·∫£n
        generate_config = types.GenerateContentConfig(
            safety_settings=safety_settings,
            temperature=0,  # Gi·ªØ m·ª©c th·∫•p ƒë·ªÉ d·ªãch ch√≠nh x√°c
        )

        base_delay = 5  # Gi√¢y ch·ªù c∆° b·∫£n

        for attempt in range(1, self.max_retries + 1):
            try:
                log(f"CALL API attempt {attempt}/{self.max_retries} | model={model}")

                # G·ªåI SDK C·ª¶A GOOGLE
                response = self.client.models.generate_content(
                    model=model,
                    contents=prompt,
                    config=generate_config
                )

                # =========================================================
                # DEBUG LOGIC: LOG CHI TI·∫æT KHI API TR·∫¢ V·ªÄ R·ªñNG
                # =========================================================
                if not response.text:
                    print(f"\n‚ùå [DEBUG ERROR] Chunk g√¢y l·ªói (Attempt {attempt}):")
                    # In snippet ƒë·ªÉ debug
                    print(f"--- INPUT SNIPPET ---\n...{prompt[-300:]}\n---------------------")

                    finish_reason = "Unknown"
                    if response.candidates:
                        c = response.candidates[0]
                        finish_reason = c.finish_reason
                        if hasattr(c, 'safety_ratings'):
                            print(f"‚ö†Ô∏è Safety Ratings: {c.safety_ratings}")

                    # N·∫øu h·∫øt l∆∞·ª£t -> B√°o l·ªói ƒë·ªÉ k√≠ch ho·∫°t Fallback ·ªü t·∫ßng tr√™n
                    if attempt == self.max_retries:
                        raise RuntimeError(f"API tr·∫£ v·ªÅ n·ªôi dung r·ªóng. Reason: {finish_reason}")

                    raise ValueError("Empty response (triggering retry)")
                # =========================================================

                # L·∫§Y TEXT
                text = response.text.strip()
                return text

            # --- X·ª¨ L√ù L·ªñI 429 (QUOTA EXHAUSTED) ---
            except errors.ClientError as e:
                if e.code == 429:
                    if attempt == self.max_retries:
                        log(f"‚ùå API ERROR: 429 Quota exhausted for model {model}.")
                        raise e  # N√©m l·ªói ra ƒë·ªÉ trigger fallback

                    # Exponential Backoff: Ch·ªù l√¢u h∆°n sau m·ªói l·∫ßn l·ªói
                    wait_time = (base_delay * (2 ** (attempt - 1))) + random.uniform(1, 3)
                    print(f"‚ö†Ô∏è [429 Quota] Model {model} b·ªã gi·ªõi h·∫°n. ƒêang ch·ªù {wait_time:.1f}s... (L·∫ßn {attempt})")
                    time.sleep(wait_time)
                else:
                    log(f"API CLIENT ERROR: {e}")
                    if attempt == self.max_retries:
                        raise e
                    time.sleep(2)

            # --- X·ª¨ L√ù L·ªñI KH√ÅC (M·∫°ng, Server 500...) ---
            except Exception as e:
                print(f"[DEBUG] L·ªói th·∫≠t s·ª±: {e}")
                log(f"API ERROR: {e}")
                if attempt == self.max_retries:
                    raise e
                time.sleep(2 * attempt)

        raise RuntimeError("Max retries exceeded")

    # =========================================================
    # COMPATIBILITY LAYER (C·∫¶U N·ªêI CHO MAIN.PY)
    # =========================================================
    def _call_openai(self, *, prompt: str, model: str) -> str:
        """
        H√†m t∆∞∆°ng th√≠ch ng∆∞·ª£c: main.py g·ªçi h√†m n√†y ƒë·ªÉ t·∫°o Glossary.
        Logic: Th·ª≠ Primary (Lite) tr∆∞·ªõc -> L·ªói -> Fallback (Flash 2.0).
        """
        try:
            return self._call_gemini_native(prompt=prompt, model=self.model_primary)
        except Exception as e:
            log(f"‚ö†Ô∏è GLOSSARY PRIMARY FAILED: {e}")
            log(f"üîÑ SWITCHING GLOSSARY TO FALLBACK: {self.model_fallback}")
            return self._call_gemini_native(prompt=prompt, model=self.model_fallback)

    # =========================================================
    # PUBLIC API ‚Äî TRANSLATE CHUNK
    # =========================================================
    def translate_chunk(
            self,
            *,
            en_blocks: List[str],
            glossary_rules: str = "",
            summary: str = "",
            characters: Optional[str] = None,  # JSON STRING from main.py
            intra_chapter_context: Optional[List[str]] = None,
            is_narrative: bool = False,
            chunk_index: Optional[int] = None,
            total_chunks: Optional[int] = None,
            total_chapter_blocks: int = 0,  # <--- 1. TH√äM THAM S·ªê N√ÄY
    ) -> List[str]:
        """
        Translate a list of English blocks into Vietnamese.
        LOGIC: Th·ª≠ Model Primary -> L·ªói -> Fallback sang Model Secondary.
        """

        kind = "NARRATIVE" if is_narrative else "NON_NARRATIVE"
        N = len(en_blocks)

        # Logic hi·ªÉn th·ªã Chunk Index: 1/?, 2/?, ho·∫∑c 1/10 n·∫øu bi·∫øt t·ªïng
        if chunk_index and total_chunks and total_chunks > 0:
            chunk_info = f"{chunk_index}/{total_chunks}"
        elif chunk_index:
            chunk_info = f"{chunk_index}/?"
        else:
            chunk_info = "?"

        # 2. LOGIC HI·ªÇN TH·ªä S·ªê BLOCK (VD: 30/150)
        blocks_info = f"{N}"
        if total_chapter_blocks > 0:
            blocks_info = f"{N}/{total_chapter_blocks}"

        log(
            f"AI TRANSLATE CHUNK | type={kind} | "
            f"chunk={chunk_info} | blocks={blocks_info} | "  # <--- 3. C·∫¨P NH·∫¨T LOG ·ªû ƒê√ÇY
            f"intra_ctx_blocks={len(intra_chapter_context or [])}"
        )

        numbered_blocks: List[str] = []
        for i, block in enumerate(en_blocks, start=1):
            numbered_blocks.append(f"[{i}] {block}")

        numbered_text = "\n".join(numbered_blocks)

        intra_context_text = ""
        if intra_chapter_context:
            trimmed_ctx = intra_chapter_context[-200:]
            intra_context_text = (
                    "INTRA-CHAPTER CONTEXT (REFERENCE ONLY):\n"
                    "The following text is from PREVIOUS translated blocks.\n"
                    "Use ONLY for tone, terminology, pronouns, and flow.\n"
                    "DO NOT translate, repeat, or continue it.\n\n"
                    + "\n".join(trimmed_ctx)
                    + "\n\n"
            )

        if is_narrative:
            role = "You are a professional literary translator."
            extra_rules = ""
        else:
            role = "You are a translation engine."
            extra_rules = (
                "This is NON-NARRATIVE content.\n"
                "Translate literally.\n"
                "Do NOT embellish or interpret.\n"
            )

        pronoun_rules = ""
        if is_narrative and characters:
            try:
                character_list = json.loads(characters)
                pronoun_rules = build_pronoun_rules(character_list)
            except Exception:
                raise RuntimeError("INVALID CHARACTER CONTEXT: cannot parse pronoun rules")

        prompt = f"""
{role}

TARGET LANGUAGE:
Vietnamese.

You MUST translate ALL content into Vietnamese.

{extra_rules}

{glossary_rules}

{pronoun_rules}

GLOBAL CONTEXT (if provided):
Summary:
{summary}

{intra_context_text}

STRICT RULES (MANDATORY ‚Äî VIOLATION = INVALID OUTPUT):

FORMAT RULES:
- Input text contains NUMBERED blocks.
- EACH numbered block MUST produce EXACTLY ONE output line.
- EVEN IF a block is very short, it MUST still have its own output line.
- DO NOT merge, combine, summarize, or infer across blocks.
- DO NOT split blocks.
- DO NOT add or remove lines.
- Output MUST contain EXACTLY {N} lines.
- Each output line MUST start with the SAME block number as input.

NO META OUTPUT (ABSOLUTE):
- You MUST output ONLY translation lines.
- You MUST NOT add notes, explanations, confirmations, or commentary.
- You MUST NOT include text such as:
  "Note:", "Explanation:", "Here is", "I have", "I followed", or similar.

- ANY line that does NOT start with a block number [i] is INVALID.


INPUT:
{numbered_text}

OUTPUT FORMAT (EXACT):
[1] <Vietnamese translation>
[2] <Vietnamese translation>
...
""".strip()

        vi_text = ""
        try:
            vi_text = self._call_gemini_native(prompt=prompt, model=self.model_primary)
        except Exception as e:
            log(f"‚ö†Ô∏è PRIMARY MODEL ({self.model_primary}) FAILED: {e}")
            log(f"üîÑ SWITCHING TO FALLBACK MODEL: {self.model_fallback}")
            try:
                vi_text = self._call_gemini_native(prompt=prompt, model=self.model_fallback)
            except Exception as e_fallback:
                log(f"‚ùå FALLBACK MODEL FAILED: {e_fallback}")
                raise e_fallback

        pattern = re.compile(r"\[(\d+)\]\s*(.*?)\s*(?=\[\d+\]|$)", re.S)
        matches = pattern.findall(vi_text)

        if not matches:
            log("=== RAW MODEL OUTPUT BEGIN ===")
            log(vi_text)
            log("=== RAW MODEL OUTPUT END ===")
            raise RuntimeError("INVALID OUTPUT: no indexed blocks found")

        vi_blocks: List[str] = []
        for idx_str, content in matches:
            vi_blocks.append(content.strip())

        if len(vi_blocks) != len(en_blocks):
            log("=== RAW MODEL OUTPUT BEGIN ===")
            log(vi_text)
            log("=== RAW MODEL OUTPUT END ===")
            raise RuntimeError(f"BLOCK COUNT MISMATCH: {len(en_blocks)} EN vs {len(vi_blocks)} VI")

        log(f"AI TRANSLATE CHUNK | success | type={kind} | blocks={len(vi_blocks)}")
        return vi_blocks